<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    
    <title>DANE Study - Server-side Artifacts</title>
    <base href="https://dane-study.github.io">
    
    
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="./css/style.css">
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,400i,700" rel="stylesheet">
    
    
    
    
    

    <noscript>
      <link rel="stylesheet" type="text/css" href="./css/noscript.css">
    </noscript>
  </head>
  <body class="container">
    <nav class="container-fluid navbar navbar-default">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
    <span class="sr-only">Toggle navigation</span>
    <span class="icon-bar">&nbsp;</span>
    <span class="icon-bar">&nbsp;</span>
    <span class="icon-bar">&nbsp;</span>
    </button>
    <a class="navbar-brand" href="#">DANE Study</a>
  </div>
  <div class="collapse navbar-collapse" id="navbar-collapse">
    <ul class="nav navbar-nav">
        <li><a href="/">Home</a></li>
        <li class="active"><a href="/server-side/">Server-side Artifacts</a></li>
        <li><a href="/client-side/">Client-side Artifacts</a></li>
        <li><a href="/contact/">Contact</a></li>
        <li><a href="/criticism/">Criticism</a></li>
    </ul>
    <ul class="nav navbar-nav navbar-right">
    </ul>
  </div>
</nav>

    

    
    <noscript>
  <div class="alert alert-warning" role="alert">
    <strong>JavaScript disabled!</strong> This page requires JavaScript, you might not be able to access all content with JavaScript disabled.
  </div>
</noscript>

    <main class="container-fluid">
      

<h1 id="how-to-reproduce-all-of-the-figures-on-the-paper-section-4-and-5">How to reproduce all of the figures on the paper? (Section 4 and 5)?</h1>

<p>Our paper is based on the measurement: TLSA records and their certificates.
We had collected them over 4 months across 5 vantage points. Due to the massive size of these datasets, we used <a href="https://spark.apache.org/">Apache Spark</a> to process them in a parallel manner.  Here, we would like to provide three approaches to reproduce our measurement results:</p>

<ul>
<li><p>First, you can (1) run our measurement source codes to collect your own datasets, (2) analyze the raw datasets you have obtained, and (3) run our plotting scripts. As the dataset will not be exactly as same as the ones we used the paper, your figures will look different. Thus, we recommend this approach for the researchers who are interested in extending our work. <em>For the ones who are interested in this approach, you can start this from the Section 3 (, 2, and 1 in a reverse order).</em></p></li>

<li><p>Second, you can (1) download our raw datasets (thus, not re-running our measurement source codes), (2) run our analysis scripts, and (3) run our plotting scripts. As you do not need to run our measurement scripts to collect your own datasets, it should be faster than the first approach. However, our dataset might be too big to run as it spans four months and are collected from five vantage points. Thus, depending on your computational resource, it may take several hours (or days) to run our analysis scripts. For your information, we used Spark clusters to efficiently analyze the datasets. <em>For the ones who are interested in this approach, you can skip the Section 3 and start this page from Section 2 and 1.</em></p></li>

<li><p>Finally, you can (1) just use our analytics datasets, which are processed through our analysis codes based on the raw datasets. We believe this way to be the fastest way that you can check the consistency of the figures on the paper. <em>For the ones who are interested in this approach, you only need to read Section 1.</em></p></li>
</ul>

<h1 id="1-reproducing-the-figures-from-the-analytics">1. Reproducing the figures from the analytics</h1>

<p>This section introduces a very simple way to reproduce all of the figures in the paper by using the analytics datasets and plotting scripts.</p>

<h2 id="datasets-and-scripts">Datasets and scripts</h2>

<h3 id="1-analytic-datasets-for-figures-and-their-gnuplot-scripts">(1) Analytic datasets for figures and their gnuplot scripts</h3>






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Download</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>Analytics</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/analytics_data.html">link</a></td>
<td>Input datasets for the figures on the paper.</td>
</tr>

<tr>
<td><code>plotting-scripts.tar.gz</code></td>
<td><a href="/codes/plotting-scripts.tar.gz">link</a></td>
<td>Plotting scripts for 6 figures.</td>
</tr>
</tbody>
</table>
<style>
table, th, td {
  text-align: center;
}
</style>


<h3 id="2-details-of-the-gnuplot-scripts">(2) Details of the gnuplot scripts</h3>






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Figure No. on the paper</th>
<th>Input data</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>2years-tlsa-ratio-per-tld-split.plot</code></td>
<td>Figure 2</td>
<td><code>tlsa-counts.csv</code></td>
</tr>

<tr>
<td><code>alexa-tlsa-adoption.plot</code></td>
<td>Figure 3</td>
<td>alexa_dane_stat_output.txt which is included in <code>alexa_dane_stat_output.tar.gz</code></td>
</tr>

<tr>
<td><code>2years-tlsa-ratio-per-tld-split-fallback.plot</code></td>
<td>Figure 4</td>
<td><code>tlsa-counts.csv</code></td>
</tr>

<tr>
<td><code>missing-dnssec.plot</code></td>
<td>Figure 5</td>
<td>dnssec_stat<em>output</em>[<em>city</em>].txt which is included in <code>dnssec_stat_output.tar.gz</code></td>
</tr>

<tr>
<td><code>startls-availability.plot</code></td>
<td>Figure 6</td>
<td>starttls_error_stat<em>output</em>[<em>city</em>].txt which is included in <code>starttls_error_stat_output.tar.gz</code></td>
</tr>

<tr>
<td><code>incorrect-percent-per-comp.plot</code></td>
<td>Figure 7</td>
<td>check_incorrect_stat<em>output</em>[<em>city</em>].txt which is included in <code>check_incorrect_stat_output.tar.gz</code></td>
</tr>

<tr>
<td><code>4months-valid-per-tld.plot</code></td>
<td>Figure 8</td>
<td>valid_dn_stat<em>output</em>[<em>city</em>].txt which is included in <code>valid_dn_stat_output.tar.gz</code></td>
</tr>
</tbody>
</table>
<style>
table, th, td {
  text-align: center;
}
</style>


<h1 id="2-reproducing-the-analytics-from-the-raw-datasets-our-measurement-datasets">2. Reproducing the analytics from the raw datasets (our measurement datasets)</h1>

<p>This section introduces a way to generate the datasets (in the <code>Analytics</code> file) from the raw datasets that we had collected using our measurement codes.
After executing the analysis scripts you may use those output files as inputs to the above plotting scripts.</p>

<h2 id="datasets-and-scripts-1">Datasets and scripts</h2>

<h3 id="1-raw-measurement-datasets-and-prerequisites-for-the-analysis">(1) Raw (measurement) datasets and prerequisites for the analysis</h3>






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Download</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>hourly snapshots</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/raw.html">link to datasets</a></td>
<td>TLSA records and their certificates (through STARTTLS) collected for 4 month (July ~ October, 2019) on the five EC2 vantage points (Virginia, Oregon, Paris, Sydney, and SÃ£o Paulo).</td>
</tr>

<tr>
<td><code>tlsa-domains-seeds.tar.gz</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/tlsa-domains-seeds.tar.gz">link</a></td>
<td>Domain names who have TLSA records for July 10st, 2019 and October 31st, 2019, which are used in <code>rollover-candidate.py</code></td>
</tr>

<tr>
<td><code>mx-with-tlsa.tar.gz</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/mx-with-tlsa.tar.gz">link</a></td>
<td>A list of MX records that have TLSA records as well. This dataset is measured at <a href="https://openintel.nl">OpenINTEL</a>.</td>
</tr>

<tr>
<td><code>alexa-top1m.csv</code></td>
<td><a href="https://toplists.net.in.tum.de/archive/alexa/alexa-top1m-2019-10-31_0900_UTC.csv.xz">link</a></td>
<td>Alexa 1M domain names captured at October 31st, 2019. This dataset is obtained from the <a href="https://toplists.github.io">top-lists study</a>.</td>
</tr>

<tr>
<td><code>alexa1m-mx.tar.gz</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/alexa1m-mx.tar.gz">link</a></td>
<td>Alexa 1M domain names that have MX records (measured at October 31st, 2019).</td>
</tr>

<tr>
<td><code>alexa1m-tlsa.tar.gz</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/alexa1m-tlsa.tar.gz">link</a></td>
<td>Alexa 1M domains that have TLSA records (measured at October 31st, 2019).</td>
</tr>

<tr>
<td><code>root-ca-list.tar.gz</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/root-ca-list.tar.gz">link</a></td>
<td>A list of root CA&rsquo;s certificates for verifying certificates.</td>
</tr>

<tr>
<td><code>Intermediary data</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/intermediary_data.html">link</a></td>
<td>Intermediary datasets which are outputs of Spark scripts and also can be used as an input for analysis scripts</td>
</tr>
</tbody>
</table>
<style>
table, th, td {
  text-align: center;
}
</style>


<h3 id="2-scripts-for-the-analysis">(2) Scripts for the analysis</h3>






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Download</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>dependencies.zip</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/dependencies.tar.gz">link</a></td>
<td>It includes our crafted python <code>dns</code> package for the Spark scripts.</td>
</tr>

<tr>
<td><code>raw-merge.py</code></td>
<td><a href="/codes/raw-merge.py">link</a></td>
<td>For the sake of simplicity, we merge the raw-datasets collected from the five vantage points into one single dataset.</td>
</tr>

<tr>
<td><code>spark-codes.tar.gz</code></td>
<td><a href="/codes/spark-codes.tar.gz">link</a></td>
<td>It includes pySpark scripts for our analysis.</td>
</tr>

<tr>
<td><code>stats-codes.tar.gz</code></td>
<td><a href="/codes/stats-codes.tar.gz">link</a></td>
<td>It includes python scripts for our analysis.</td>
</tr>
</tbody>
</table>
<style>
table, th, td {
  text-align: center;
}
</style>


<h2 id="how-to-use-the-datasets-and-scripts">How to use the datasets and scripts?</h2>

<h3 id="1-preprocessing-the-raw-datasets">(1) Preprocessing the raw datasets.</h3>

<p>We had collected two raw datasets: TLSA records (via DNS) and their certificates (via STARTTLS).
To use DANE correctly, these two objects have to be matched; thus, we read these two datasets using <code>raw-mergy.py</code> and generates the output as a JSON format.
After downloading the <code>hourly snapshots</code>, configure the input and output path (global variable in the script) for a city <br>(e.g., virginia) you want to merge and run <code>raw-merge.py</code>.</p>

<pre><code>python3 raw-merge.py 190711 191031 
</code></pre>

<p>After execution, merged outputs are placed in the <code>merged_output/[city]/</code> directory.</p>

<p>Below JSON data is an example of a merged output.</p>

<pre><code>...
{
  &quot;domain&quot;: &quot;mail.ietf.org.&quot;,
  &quot;port&quot;: &quot;25&quot;,
  &quot;time&quot;: &quot;20191031 9&quot;,
  &quot;city&quot;: &quot;virginia&quot;, 
  &quot;tlsa&quot;: {
  	    &quot;dnssec&quot;: &quot;Secure&quot;, // DNSSEC validation result
  	    &quot;record_raw&quot;: &quot;AACBoAABAAIABwABA18yNQRfdGNwBG1haWwEaWV0ZgNvcmcAADQAAQNfMjUEX3...&quot; // DNS wire-format TLSA record, Base64 Encoded
  	    },

  &quot;starttls&quot;: {
  	    &quot;certs&quot;: &quot;[&quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUdWekNDQlQrZ0F3SUJBZ...&quot;, // PEM format certificate, Base64 Encoded
  	    	       &quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZBRENDQStpZ0F3SUJBZ...&quot;,
  	    	       &quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVvRENDQTRpZ0F3SUJBZ...&quot;]
  }
}
...
</code></pre>

<p>Now you are ready to run the Spark scripts.</p>

<h3 id="2-analyzing-the-merged-datasets">(2) Analyzing the merged datasets</h3>

<p>Apache Spark is specialized for big data processing using multiple cores at the same time. However, it may not work efficiently when the datasets have many dependencies between themselves. Thus, we first use Spark to extract the information that we are interested in from our raw datasets and we run a (analysis) python script to analyze them in depth.</p>

<p>The below table shows which Spark, analysis, and gnuplot scripts are used to get results in the paper.</p>






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Result</th>
<th>Spark</th>
<th>Analysis</th>
<th>Gnuplot script</th>
</tr>
</thead>

<tbody>
<tr>
<td>Figure 2</td>
<td>-</td>
<td>-</td>
<td><code>2years-tlsa-ratio-per-tld-split.plot</code></td>
</tr>

<tr>
<td>Figure 3</td>
<td>-</td>
<td><code>alexa1m-dane-stat.py</code></td>
<td><code>alexa-tlsa-adoption.plot</code></td>
</tr>

<tr>
<td>Figure 4</td>
<td>-</td>
<td>-</td>
<td><code>2years-tlsa-ratio-per-tld-split-fallback.plot</code></td>
</tr>

<tr>
<td>Figure 5</td>
<td><code>dnssec.py</code></td>
<td><code>dnssec-stat.py</code></td>
<td><code>missing-dnssec.plot</code></td>
</tr>

<tr>
<td>Figure 6</td>
<td><code>starttls-error.py</code></td>
<td><code>starttls-error-stat.py</code></td>
<td><code>starttls-availability.plot</code></td>
</tr>

<tr>
<td>Figure 7</td>
<td><code>check-incorrect.py</code></td>
<td><code>check-incorrect-stat.py</code></td>
<td><code>incorrect-percent-per-comp.plot</code></td>
</tr>

<tr>
<td>Figure 8</td>
<td><code>valid-dn.py</code></td>
<td><code>valid-dn-stat.py</code></td>
<td><code>4months-valid-per-tld.plot</code></td>
</tr>

<tr>
<td>Section 5.5</td>
<td><code>rollover.py</code></td>
<td><code>rollover-stat.py</code></td>
<td>-</td>
</tr>
</tbody>
</table>
<style>
table, th, td {
  text-align: center;
}
</style>


<p>For example, to get the input dataset for Figure 5, run Spark script <code>dnssec.py</code>. Next, run the analysis script <code>dnssec-stat.py</code> using the output of <code>dnssec.py</code> as an input. Finally, you can draw Figure 5 with the <code>missing-dnssec.plot</code> script using the output of <code>dnssec-stat.py</code> as an input.</p>

<h4 id="running-spark-scripts">Running Spark scripts</h4>

<p>The <code>spark-codes.tar.gz</code> file contains nine Spark scripts that run on a Spark machine. Please note that these scripts take the <em>merged data</em> as an input.
We use a third-party library that we crafted, <a href="http://www.dnsypython.org">dns</a>. You may want to install this library on the Spark machine or you can pass this library to the machine when you run the Spark code by using the <code>--py-files</code> option. For the sake of simplicity, we have provided a package, <code>dependencies.zip</code>.</p>

<pre><code>spark-submit --py-files=/path/to/dependencies.zip [spark_script.py]
</code></pre>

<p>The below table describes each of the Spark script that we use for the analyses.</p>






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Description</th>
<th>Input</th>
<th>Output (same as the ones in <a href="https://mmlab.snu.ac.kr/~hmlee/dane/intermediary_data.html">Intermediary data</a>)</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>chain-validation.py</code></td>
<td>It verifies a chain of certificates</td>
<td><code>root-ca-list</code></td>
<td><code>chain_validation_output.tar.gz</code></td>
</tr>

<tr>
<td><code>dane-validation.py</code></td>
<td>It validates DANE based on <a href="https://tools.ietf.org/html/rfc7671">RFC7671</a></td>
<td>the output of <code>chain-validation.py</code><br>(e.g., chain_output_virginia/ in <code>chain_validation_output.tar.gz</code>)</td>
<td><code>dane_validation_output.tar.gz</code></td>
</tr>

<tr>
<td><code>dnssec.py</code></td>
<td>It checks DNSSEC validity of TLSA records</td>
<td>-</td>
<td><code>dnssec_output.tar.gz</code></td>
</tr>

<tr>
<td><code>starttls-error.py</code></td>
<td>It classifies the reasons of STARTTLS scanning failure</td>
<td>-</td>
<td><code>starttls_error_output.tar.gz</code></td>
</tr>

<tr>
<td><code>check-incorrect.py</code></td>
<td>It classify the reasons of DANE validation failure</td>
<td>-</td>
<td><code>check_incorrect_output.tar.gz</code></td>
</tr>

<tr>
<td><code>rollover-candidate.py</code></td>
<td>It extracts the domains who have conducted rollover</td>
<td><code>tlsa-base-domains-seeds</code></td>
<td><code>rollover_candidate_output.tar.gz</code></td>
</tr>

<tr>
<td><code>rollover.py</code></td>
<td>It evaluates rollover behavior of domains</td>
<td>the output of <code>rollover-candidate-sub.py</code><br>(e.g., rollover-cand-merged-virginia.txt in <code>rollover_cand_merged_output.tar.gz</code>)</td>
<td><code>rollover_output.tar.gz</code></td>
</tr>

<tr>
<td><code>valid-dn.py</code></td>
<td>It counts the number of domains associated with mail servers which have valid TLSA records</td>
<td>the output of <code>dane-validation.py</code><br>(e.g., dane_output_virginia/ in <code>dane_validation_output.tar.gz</code>) &amp; <code>mx-with-tlsa</code></td>
<td><code>valid_dn_output.tar.gz</code></td>
</tr>
</tbody>
</table>
<style>
table, th, td {
  text-align: center;
}
</style>


<h4 id="running-analysis-scripts">Running analysis scripts</h4>

<p>After getting outputs from the Spark scripts, you can analyze those outputs. <code>stats-codes.tar.gz</code> contains 9 analysis scripts for this purpose. The output files must be same as the ones in the <a href="https://mmlab.snu.ac.kr/~hmlee/dane/analytics_data.html">Analytics</a>.</p>






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Description</th>
<th>Input</th>
<th>Output</th>
<th>Misc.</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>dane-validation-stat.py</code></td>
<td>It calculates stats of dane validation results</td>
<td>the output of <code>dane-validation.py</code><br>(e.g., dane<em>output</em>[<em>city</em>]/ in <code>dane_validation_output.tar.gz</code>)</td>
<td><code>dane_valid_stat_output.tar.gz</code></td>
<td>-</td>
</tr>

<tr>
<td><code>dnssec-stat.py</code></td>
<td>It calculates stats of dnssec validation results</td>
<td>the output of <code>dnssec.py</code><br>(e.g., dnssec<em>output</em>[<em>city</em>]/ in <code>dnssec_output.tar.gz</code>)</td>
<td><code>dnssec_stat_output.tar.gz</code></td>
<td>-</td>
</tr>

<tr>
<td><code>starttls-error-stat.py</code></td>
<td>It calculates the stats of STARTTLS scanning errors</td>
<td>the output of <code>starttls-error.py</code><br>(e.g., starttls_error<em>output</em>[<em>city</em>]/ in <code>starttls_error_output.tar.gz</code>)</td>
<td><code>starttls_error_stat_output.tar.gz</code></td>
<td>-</td>
</tr>

<tr>
<td><code>check-incorrect-stat.py</code></td>
<td>It calculates stats of dane validation failure reasons</td>
<td>the output of <code>check-incorrect.py</code><br>(e.g, incorrect<em>output</em>[<em>city</em>]/ in <code>check_incorrect_output.tar.gz</code>)</td>
<td><code>check_incorrect_stat_output.tar.gz</code></td>
<td>-</td>
</tr>

<tr>
<td><code>rollover-candidate-sub.py</code></td>
<td>It finds the domains who have conducted rollover</td>
<td>the output of <code>rollover-candidate.py</code><br>(e.g., rollover_cand_output_virginia/ in <code>rollover_candidate_output.tar.gz</code>)</td>
<td><code>rollover_cand_merged_output.tar.gz</code></td>
<td>-</td>
</tr>

<tr>
<td><code>rollover-stat.py</code></td>
<td>It calculates stats of rollover behavior of domains</td>
<td>the output of <code>rollover.py</code><br>(e.g., rollover_output_virginia/ in <code>rollover_output.tar.gz</code>)</td>
<td><code>rollover_stat_output.tar.gz</code></td>
<td>Section 5.5, Key Rollover</td>
</tr>

<tr>
<td><code>valid-dn-stat.py</code></td>
<td>It calculates stats of DANE-valid domains for each TLD</td>
<td>the output of <code>valid-dn.py</code><br>(e.g., valid_dn_virginia/ in <code>valid_dn_output.tar.gz</code>)</td>
<td><code>valid_dn_stat_output.tar.gz</code></td>
<td>-</td>
</tr>

<tr>
<td><code>alexa1m-dane-stat.py</code></td>
<td>It calculates stats of Alexa 1M domains who have TLSA records</td>
<td><code>alexa1m-mx</code>, <code>alexa1m-tlsa</code>, <code>alexa-top1m.csv</code></td>
<td><code>alexa_dane_stat_output.tar.gz</code></td>
<td>-</td>
</tr>
</tbody>
</table>
<style>
table, th, td {
  text-align: center;
}
</style>


<h1 id="3-running-our-measurement-codes-to-get-your-own-raw-datasets-tlsa-records-and-their-certificates">3. Running our measurement codes to get your own raw datasets (TLSA records and their certificates)</h1>

<p>This section introduces our source codes that we used to collect our datasets. We used these source codes to collect TLSA records and their certificates by sending average 11,972 TLSA record lookups as well as the certificate chains every hour from July 11, 2019 to October 31, 2019. We refer to these measurements as the Hourly dataset.</p>

<p>What about Daily dataset? Because the Daily dataset that contains every domain names under top level domains was collected using zone files that are given under agreement with registries, we cannot make them just publicly available. Instead, we provide intermediary data <br>(e.g. <code>tlsa-counts.csv</code>) extracted from the Daily dataset which is needed to run our scripts.</p>

<h2 id="scanning-source-codes">Scanning source codes</h2>

<p>




<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Descrption</th>
<th>Download</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>tlsa-scan.go</code></td>
<td>It fetches TLSA records from a list of domains</td>
<td><a href="/codes/tlsa-scan.go">link</a><sup>1</sup></td>
</tr>

<tr>
<td><code>starttls-scan.go</code></td>
<td>It collects certificates via STARTTLS</td>
<td><a href="/codes/starttls-scan.go">link</a></td>
</tr>
</tbody>
</table>
<style>
table, th, td {
  text-align: center;
}
</style>

<sup>1</sup> This requires the following third-party libraries: <a href="https://nlnetlabs.nl/projects/unbound/about/">Unbound</a>,  <a href="https://github.com/miekg/unbound">Unbound Golang Wrapper</a>, and <a href="https://nlnetlabs.nl/projects/ldns/about/">ldns</a>.</p>

<h2 id="how-to-scan-tlsa-records-and-their-certificates">How to scan TLSA records and their certificates</h2>

<h3 id="1-scan-tlsa-records">1. Scan TLSA records</h3>

<p>The script <code>tlsa-scan.go</code> will read a list of domains in the <code>mx-with-tlsa</code> file and collect their TLSA records. The output has the following format.</p>






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>TLSA-base-domain</th>
<th>Vantage point</th>
<th>DNSSEC validity <sup>1</sup></th>
<th>TLSA records<sup>2</sup></th>
</tr>
</thead>

<tbody>
<tr>
<td>_25._tcp.mail.ietf.org.</td>
<td>Virginia</td>
<td>Secure</td>
<td>AACBoAABAAIAB&hellip;</td>
</tr>

<tr>
<td>_25._tcp.mail.tutanota.de.</td>
<td>Virginia</td>
<td>Secure</td>
<td>AACBoAABAAIA&hellip;</td>
</tr>

<tr>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>
<style>
table, th, td {
  text-align: center;
}
</style>


<p><sup>1</sup> The result of DNSSEC validation: <code>Secure</code> indicates that a domain can be validated. <code>Insecure</code> indicates that a domain cannot be validated because it does not have a DS record. <code>Bogus</code> indicates that a domain cannot be validated because it has invalid DNSSEC records such as expired RRSIGs.</p>

<p><sup>2</sup> TLSA records: wire formatted TLSA records (base64 encoded)</p>

<h3 id="2-scan-starttls-certificates">2. Scan STARTTLS certificates</h3>

<p>The script <code>starttls-scan.go</code> will read a list of domains in the <code>mx-with-tlsa</code> file and collect certificates presented via STARTTLS. The output has the following format.</p>






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Domains</th>
<th>Port</th>
<th>Vantage point</th>
<th>Status<sup>1</sup></th>
<th># of presented certificates<sup>2</sup></th>
<th>Certificates<sup>3</sup></th>
</tr>
</thead>

<tbody>
<tr>
<td>mail.ietf.org</td>
<td>25</td>
<td>Virginia</td>
<td>Success</td>
<td>4</td>
<td>LS0RUaAB&hellip;, WjGdVBWYi&hellip;, 0s3FTFRuZ1&hellip;, eFKdDRBO&hellip;</td>
</tr>

<tr>
<td>mail.tutanota.de.</td>
<td>25</td>
<td>Virginia</td>
<td>Success</td>
<td>4</td>
<td>LSSf7JanC&hellip;, ODlF4NEF&hellip;, SA3S29K&hellip;, Z1RstKS&hellip;</td>
</tr>

<tr>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>
<style>
table, th, td {
  text-align: center;
}
</style>


<p><sup>1</sup> Whether a certificate has been successfully fetched or not.</p>

<p><sup>2</sup> The number of certificates presented via STARTTLS.</p>

<p><sup>3</sup> A list of base64 encoded (PEM format) certificates (each certificate is comma seperated).</p>

    </main>
    <footer class="container-fluid page-footer" style="display: flex; align-items: center">
</footer>

    
<script type="text/javascript">
var sc_project=11603023; 
var sc_invisible=1; 
var sc_security="a906280a"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11603023/0/a906280a/1/" alt="Web
Analytics"></a></div></noscript>


    
    <script src="js/jquery.1.12.4.min.js"></script>
    
    <script src="js/bootstrap.min.js"></script>
    <script src="js/script.js"></script>
  </body>
</html>
