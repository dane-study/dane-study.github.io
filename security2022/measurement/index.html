<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    
    <title>DANE Study - Measurement Artifacts</title>
    <base href="https://dane-study.github.io">
    
    
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="./css/style.css">
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,400i,700" rel="stylesheet">
    
    
    
    
    

    <noscript>
      <link rel="stylesheet" type="text/css" href="./css/noscript.css">
    </noscript>
  </head>
  <body class="container">
    <nav class="container-fluid navbar navbar-default">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
    <span class="sr-only">Toggle navigation</span>
    <span class="icon-bar">&nbsp;</span>
    <span class="icon-bar">&nbsp;</span>
    <span class="icon-bar">&nbsp;</span>
    </button>
    <a class="navbar-brand" href="#">DANE Study</a>
  </div>
  <div class="collapse navbar-collapse" id="navbar-collapse">
    <ul class="nav navbar-nav">
        <li><a href="/">Home</a></li>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">USENIX Security 2020<span class="caret">&nbsp;</span></a>
          <ul class="dropdown-menu">
            <li><a href="/security2020/security2020-abstract">Abstract</a></li>
            <li><a href="/security2020/server-side">Server-side Artifacts</a></li>
            <li><a href="/security2020/client-side">Client-side Artifacts</a></li>
            <li><a href="/security2020/critics">Critics</a></li>
            <li><a href="/security2020/contact">Contact</a></li>
          </ul>
        </li>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">USENIX Security 2022<span class="caret">&nbsp;</span></a>
          <ul class="dropdown-menu">
            <li><a href="/security2022/security2022-abstract">Abstract</a></li>
            <li><a href="/security2022/measurement">Measurement Artifacts</a></li>
            <li><a href="/security2022/contact">Contact</a></li>
          </ul>
        </li>
    </ul>
    <ul class="nav navbar-nav navbar-right">
    </ul>
  </div>
</nav>

    

    
    <noscript>
  <div class="alert alert-warning" role="alert">
    <strong>JavaScript disabled!</strong> This page requires JavaScript, you might not be able to access all content with JavaScript disabled.
  </div>
</noscript>

    <main class="container-fluid">
      <h1 id="how-to-reproduce-all-of-the-figures-on-the-paper-section-4-5-and-6">How to reproduce all of the figures on the paper? (Section 4, 5, and 6)?</h1>
<p>Our paper is based on the measurement: TLSA records and their certificates.
We had collected them over 20 months. Due to the massive size of these datasets, we used <a href="https://spark.apache.org/">Apache Spark</a> to process them in a parallel manner.  Here, we would like to provide three approaches to reproduce our measurement results:</p>
<ul>
<li>
<p>First, you can (1) run our measurement source codes to collect your own datasets, (2) analyze the raw datasets you have obtained, and (3) run our plotting scripts. As the dataset will not be exactly as same as the ones we used the paper, your figures will look different. Thus, we recommend this approach for the researchers who are interested in extending our work. <em>For the ones who are interested in this approach, you can start this from the Section 3 (, 2, and 1 in a reverse order).</em></p>
</li>
<li>
<p>Second, you can (1) download our raw datasets (thus, not re-running our measurement source codes), (2) run our analysis scripts, and (3) run our plotting scripts. As you do not need to run our measurement scripts to collect your own datasets, it should be faster than the first approach. However, our dataset might be too big to run as it spans twenty months. Thus, depending on your computational resource, it may take several hours (or days) to run our analysis scripts. For your information, we used Spark clusters to efficiently analyze the datasets. <em>For the ones who are interested in this approach, you can skip the Section 3 and start this page from Section 2 and 1.</em></p>
</li>
<li>
<p>Finally, you can (1) just use our analytics datasets, which are processed through our analysis codes based on the raw datasets. We believe this way to be the fastest way that you can check the consistency of the figures on the paper. <em>For the ones who are interested in this approach, you only need to read Section 1.</em></p>
</li>
</ul>
<h1 id="1-reproducing-the-figures-from-the-analytics">1. Reproducing the figures from the analytics</h1>
<p>This section introduces a very simple way to reproduce all of the figures in the paper by using the analytics datasets and plotting scripts.</p>
<h2 id="datasets-and-scripts">Datasets and scripts</h2>
<h3 id="1-analytic-datasets-for-figures-and-their-gnuplot-scripts">(1) Analytic datasets for figures and their gnuplot scripts</h3>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Download</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Analytics</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/security2022/analytics_data.html">link</a></td>
<td>Input datasets for the figures on the paper.</td>
</tr>
<tr>
<td><code>plotting-scripts.tar.gz</code></td>
<td><a href="/codes/plotting-scripts-2022.tar.gz">link</a></td>
<td>Plotting scripts for 6 figures.</td>
</tr>
</tbody>
</table>

<style>
table, th, td {
  text-align: center;
}
</style>

<h3 id="2-details-of-the-gnuplot-scripts">(2) Details of the gnuplot scripts</h3>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Figure No. on the paper</th>
<th>Input data</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>mx-dn-serving-stat.plot</code></td>
<td>Figure 2</td>
<td>mx-dn-[valid or invalid]-serving-stat.txt which is included in <code>mx_dn_serving_stat.tar.gz</code></td>
</tr>
<tr>
<td><code>case-stat.plot</code></td>
<td>Figure 3</td>
<td>case-stat.txt which is included in <code>case_stat.tar.gz</code></td>
</tr>
<tr>
<td><code>invalid-reasons.plot</code></td>
<td>Figure 4</td>
<td>case-tlsa-stat-[SSDS or SSDO] whic is included in <code>case_tlsa_stat.tar.gz</code></td>
</tr>
<tr>
<td><code>ever-matched.plot</code></td>
<td>Figure 5</td>
<td>case-tlsa-stat-[SSDS or SSDO] whic is included in <code>case_tlsa_stat.tar.gz</code></td>
</tr>
<tr>
<td><code>le-rollover-daneee.plot</code></td>
<td>Figure 7</td>
<td>rollover-timeline-le.txt which is included in <code>rollover_timeline_le.tar.gz</code><br>and cert-pki-cn-stat.txt which is included in <code>cert_pki_cn_stat.tar.gz</code></td>
</tr>
<tr>
<td><code>le-rollover-ta.plot</code></td>
<td>Figure 8</td>
<td>rollover-timeline-le.txt which is included in <code>rollover_timeline_le.tar.gz</code><br>and cert-pki-cn-stat.txt which is included in <code>cert_pki_cn_stat.tar.gz</code></td>
</tr>
</tbody>
</table>

<style>
table, th, td {
  text-align: center;
}
</style>

<h1 id="2-reproducing-the-analytics-from-the-raw-datasets-our-measurement-datasets">2. Reproducing the analytics from the raw datasets (our measurement datasets)</h1>
<p>This section introduces a way to generate the datasets (in the <code>Analytics</code> file) from the raw datasets that we had collected using our measurement codes.
After executing the analysis scripts you may use those output files as inputs to the above plotting scripts.</p>
<h2 id="datasets-and-scripts-1">Datasets and scripts</h2>
<h3 id="1-raw-measurement-datasets-and-prerequisites-for-the-analysis">(1) Raw (measurement) datasets and prerequisites for the analysis</h3>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Download</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>hourly dataset</code></td>
<td>-<sup>1</sup></td>
<td>TLSA records and their certificates (through STARTTLS) collected for 20 month (July, 2019 ~ February, 2021)<br>on the EC2 vantage point (Virginia).</td>
</tr>
<tr>
<td><code>popularity_data.tar.gz</code></td>
<td>-<sup>1</sup></td>
<td>Popularity datasets which are used to identify managing entities of SMTP servers and name servers.</td>
</tr>
<tr>
<td><code>all-mx-exclude-nl.tar.gz</code></td>
<td>-<sup>1</sup></td>
<td>A list of all SMTP servers in our dataset.</td>
</tr>
<tr>
<td><code>root-ca-list.tar.gz</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/security2022/root-ca-list.tar.gz">link</a></td>
<td>A list of root CA&rsquo;s certificates for verifying certificates.</td>
</tr>
<tr>
<td><code>public-intermediate-certs.tar.gz</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/security2022/public-intermediate-certs.tar.gz">link</a></td>
<td>A list of intermediate CA certificates  and revoked intermediate CA certificates.<br>This data is obtained from the <a href="https://wiki.mozilla.org/CA/Intermediate_Certificates">Mozilla wiki</a>.</td>
</tr>
</tbody>
</table>
<!--`Intermediary data` | -<sup>1</sup> | Intermediary datasets which are outputs of Spark scripts and also can be used as an input for analysis scripts.-->

<style>
table, th, td {
  text-align: center;
}
</style>

<p><sup>1</sup> Due to the size of the datasets, please email us for data acesss.</p>
<h3 id="2-scripts-for-the-analysis">(2) Scripts for the analysis</h3>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Download</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>dependencies.zip</code></td>
<td><a href="https://mmlab.snu.ac.kr/~hmlee/dane/security2020/dependencies.tar.gz">link</a></td>
<td>It includes our crafted python <code>dns</code> package for the Spark scripts.</td>
</tr>
<tr>
<td><code>raw-merge.py</code></td>
<td><a href="/codes/raw-merge.py">link</a></td>
<td>For the sake of simplicity, we merge the collected raw-datasets into one single dataset.</td>
</tr>
<tr>
<td><code>spark-codes.tar.gz</code></td>
<td><a href="/codes/spark-codes-2022.tar.gz">link</a></td>
<td>It includes pySpark scripts for our analysis.</td>
</tr>
<tr>
<td><code>stats-codes.tar.gz</code></td>
<td><a href="/codes/stats-codes-2022.tar.gz">link</a></td>
<td>It includes python scripts for our analysis.</td>
</tr>
</tbody>
</table>

<style>
table, th, td {
  text-align: center;
}
</style>

<h2 id="how-to-use-the-datasets-and-scripts">How to use the datasets and scripts?</h2>
<h3 id="1-preprocessing-the-raw-datasets">(1) Preprocessing the raw datasets.</h3>
<p>We had collected two raw datasets: TLSA records (via DNS) and their certificates (via STARTTLS).
To use DANE correctly, these two objects have to be matched; thus, we read these two datasets using <code>raw-mergy.py</code> and generates the output as a JSON format.
After downloading the <code>hourly dataset</code>, configure the input and output path (global variable in the script) you want to merge and run <code>raw-merge.py</code>.</p>
<pre tabindex="0"><code>python3 raw-merge.py 190711 210212 
</code></pre><p>After execution, merged outputs (<em>merged_data</em>) are placed in the <code>[output_path]/</code> directory.</p>
<p>Below JSON data is an example of a <em>merged_data</em>.</p>
<pre tabindex="0"><code>...
{
  &quot;domain&quot;: &quot;mail.ietf.org.&quot;,
  &quot;port&quot;: &quot;25&quot;,
  &quot;time&quot;: &quot;20191031 9&quot;,
  &quot;city&quot;: &quot;virginia&quot;, 
  &quot;tlsa&quot;: {
  	    &quot;dnssec&quot;: &quot;Secure&quot;, // DNSSEC validation result
  	    &quot;record_raw&quot;: &quot;AACBoAABAAIABwABA18yNQRfdGNwBG1haWwEaWV0ZgNvcmcAADQAAQNfMjUEX3...&quot; // DNS wire-format TLSA record, Base64 Encoded
  	    },

  &quot;starttls&quot;: {
  	    &quot;certs&quot;: &quot;[&quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUdWekNDQlQrZ0F3SUJBZ...&quot;, // PEM format certificate, Base64 Encoded
  	    	       &quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZBRENDQStpZ0F3SUJBZ...&quot;,
  	    	       &quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVvRENDQTRpZ0F3SUJBZ...&quot;]
  }
}
...
</code></pre><p>Now you are ready to run the Spark scripts.</p>
<h3 id="2-analyzing-the-merged-datasets">(2) Analyzing the merged datasets</h3>
<p>Apache Spark is specialized for big data processing using multiple cores at the same time. However, it may not work efficiently when the datasets have many dependencies between themselves. Thus, we first use Spark to extract the information that we are interested in from our raw datasets and we run a (analysis) python script to analyze them in depth.</p>
<p>The below table shows which Spark, analysis, and gnuplot scripts are used to get results in the paper.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Result</th>
<th>Spark</th>
<th>Analysis</th>
<th>Gnuplot script</th>
</tr>
</thead>
<tbody>
<tr>
<td>Figure 2</td>
<td><code>mx_dn_serving.py</code></td>
<td><code>mx-dn-serving-stat.py</code></td>
<td><code>mx-dn-serving-stat.plot</code></td>
</tr>
<tr>
<td>Figure 3</td>
<td><code>case_stat.py</code></td>
<td><code>case-stat.py</code></td>
<td><code>case-stat.plot</code></td>
</tr>
<tr>
<td>Figure 4</td>
<td><code>case_tlsa_stat.py</code></td>
<td><code>case-tlsa-stat.py</code></td>
<td><code>invalid-reasons.plot	</code></td>
</tr>
<tr>
<td>Figure 5</td>
<td><code>case_tlsa_stat.py</code></td>
<td><code>case-tlsa-stat.py</code></td>
<td><code>ever-matched.plot</code></td>
</tr>
<tr>
<td>Figure 7</td>
<td><code>rollover.py</code>, <code>le_stat_spark.py</code>, <code>cert_pki_cn.py</code></td>
<td><code>rollover-timeline-le.py</code>, <code>cert-pki-cn-stat.py</code></td>
<td><code>le-rollover-daneee.plot</code></td>
</tr>
<tr>
<td>Figure 8</td>
<td><code>rollover.py</code>, <code>le_stat_spark.py</code>, <code>cert_pki_cn.py</code></td>
<td><code>rollover-timeline-le.py</code>, <code>cert-pki-cn-stat.py</code></td>
<td><code>le-rollover-ta.plot</code></td>
</tr>
<tr>
<td>Table 3</td>
<td><code>rollover.py</code>, <code>rollover_case_target.py</code></td>
<td><code>rollover-case.py</code></td>
<td>-</td>
</tr>
<tr>
<td>Table 4</td>
<td><code>init_deploy.py</code></td>
<td><code>init-deploy-stat.py</code></td>
<td>-</td>
</tr>
</tbody>
</table>

<style>
table, th, td {
  text-align: center;
}
</style>

<p>For example, to get the input dataset for Figure 3, run Spark script <code>case_stat.py</code>. Next, run the analysis script <code>case-stat.py</code> using the output of <code>case_stat.py</code> as an input. Finally, you can draw Figure 3 with the <code>case-stat.plot</code> script using the output of <code>case-stat.py</code> as an input.</p>
<h4 id="running-spark-scripts">Running Spark scripts</h4>
<p>The <code>spark-codes.tar.gz</code> file contains sixteen Spark scripts that run on a Spark machine.
We use a third-party library that we crafted, <a href="http://www.dnsypython.org">dns</a>. You may want to install this library on the Spark machine or you can pass this library to the machine when you run the Spark code by using the <code>--py-files</code> option. For the sake of simplicity, we have provided a package, <code>dependencies.zip</code>.</p>
<pre tabindex="0"><code>spark-submit --py-files=/path/to/dependencies.zip [spark_script.py]
</code></pre><p>The below table describes each of the Spark script that we use for the analyses.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Description</th>
<th>Input</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>dane_validation.py</code></td>
<td>It validates DANE based on <a href="https://tools.ietf.org/html/rfc7671">RFC7671</a>.</td>
<td><em>merged_data</em></td>
</tr>
<tr>
<td><code>check_incorrect_reason.py</code></td>
<td>It classifies the reasons for DANE validation failure.</td>
<td>the output of <code>dane_validation.py</code></td>
</tr>
<tr>
<td><code>antago_syix.py</code></td>
<td>It identifies SMTP servers that are served by Antagonist or Syix.</td>
<td><em>merged_data</em></td>
</tr>
<tr>
<td><code>cert_pki_cn.py</code></td>
<td>It identifies SMTP servers that use certificates issued by public CAs.</td>
<td><em>merged_data</em>, the output of <code>antago_syix.py</code>,<br><code>all-mx-exclude-nl.tar.gz</code></td>
</tr>
<tr>
<td><code>le_stat_spark.py</code></td>
<td>It identifies SMTP servers that use certificates issued by Let&rsquo;s Encrypt.</td>
<td><em>merged_data</em>, the output of <code>antago_syix.py</code>,<br><code>all-mx-exclude-nl.tar.gz</code></td>
</tr>
<tr>
<td><code>rollover_groupby.py</code></td>
<td>It generates groups of <em>merged_data</em> that have the same SMTP server.</td>
<td><em>merged_data</em></td>
</tr>
<tr>
<td><code>ever_matched.py</code></td>
<td>It evaluates whether mismatched TLSA records can be matched<br>with outdated certificates.</td>
<td>the output of <code>rollover_groupby.py</code></td>
</tr>
<tr>
<td><code>find_case.py</code></td>
<td>It classifies domains to each managing categories.</td>
<td>the datasets in <code>popularity_data.tar.gz</code></td>
</tr>
<tr>
<td><code>map_case.py</code></td>
<td>It merges domain data with their DANE validation results.</td>
<td>the output of <code>dane_validation.py</code> and <code>find_case.py</code></td>
</tr>
<tr>
<td><code>mx_dn_serving.py</code></td>
<td>It calculates the number of domains served by an SMTP server<br>and its DANE validity.</td>
<td>the output of <code>map_case.py</code></td>
</tr>
<tr>
<td><code>case_stat.py</code></td>
<td>It generates statistics of DANE validation results for domains<br>according to managing categories.</td>
<td>the output of <code>map_case.py</code></td>
</tr>
<tr>
<td><code>case_tlsa_stat.py</code></td>
<td>It generates statistics of DANE validation results for SMTP servers<br>according to managing categories.</td>
<td>the output of <code>dane_validation.py</code>, <code>ever_matched.py</code>,<br>and <code>map_case.py</code></td>
</tr>
<tr>
<td><code>rollover_candidate.py</code></td>
<td>It extracts the SMTP servers that have conducted rollover.</td>
<td>the output of <code>rollover_groupby.py</code>, <code>antago_syix.py</code>,<br><code>all-mx-exclude-nl.tar.gz</code></td>
</tr>
<tr>
<td><code>rollover.py</code></td>
<td>It evaluates rollover behaviors of SMTP servers.</td>
<td>the output of <code>rollover_groupby.py</code>, <code>antago_syix.py</code>,<br><code>rollover-candidate.py</code>,<br><code>never-matched.py</code>, <code>all-mx-exclude-nl.tar.gz</code></td>
</tr>
<tr>
<td><code>rollover_case_target.py</code></td>
<td>It evaluates rollovers according to managing categories.</td>
<td>the output of <code>map_case.py</code>, <code>rollover-stat.py</code></td>
</tr>
<tr>
<td><code>init_deploy.py</code></td>
<td>It evaluates the initial DANE deployment of SMTP servers.</td>
<td><em>merged_data</em>, the output of <code>aantago_syix.py</code>,<br><code>gen-init-seed.py</code>, <code>all-mx-exclude-nl.tar.gz</code></td>
</tr>
</tbody>
</table>

<style>
table, th, td {
  text-align: center;
}
</style>

<h4 id="running-analysis-scripts">Running analysis scripts</h4>
<p>After getting outputs from the Spark scripts, you can analyze those outputs. <code>stats-codes.tar.gz</code> contains eleven analysis scripts for this purpose. The output files must be same as the ones in the <a href="https://mmlab.snu.ac.kr/~hmlee/dane/security2022/analytics_data.html"><code>Analytics</code></a>.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Description</th>
<th>Input</th>
<th>Output</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cert-pki-cn-stat.py</code></td>
<td>It calculates stats of STMP servers that use certificates from public CAs.</td>
<td>the output of <code>cert_pki_cn.py</code></td>
<td><code>cert_pki_cn_stat.tar.gz</code></td>
<td></td>
</tr>
<tr>
<td><code>mx-dn-serving-stat.py</code></td>
<td>It calculates stats of the number of domains served by an SMTP server and its DANE validity.</td>
<td>the output of <code>mx_dn_serving.py</code></td>
<td><code>mx_dn_serving_stat.tar.gz</code></td>
<td></td>
</tr>
<tr>
<td><code>case-stat.py</code></td>
<td>It calculates stats of DANE validation results for domains according to managing categories.</td>
<td>the output of <code>case_stat.py</code></td>
<td><code>case_stat.tar.gz</code></td>
<td></td>
</tr>
<tr>
<td><code>case-tlsa-stat.py</code></td>
<td>It calculates stats of DANE validation results for SMTP servers according to managing categories.</td>
<td>the output of <code>case_tlsa_stat.py</code></td>
<td><code>case_tlsa_stat.tar.gz</code></td>
<td></td>
</tr>
<tr>
<td><code>never-matched.py</code></td>
<td>It finds SMTP servers that never have valid TLSA records.</td>
<td>the output of <code>dane_validation.py</code></td>
<td>-</td>
<td></td>
</tr>
<tr>
<td><code>rollover-candidate.py</code></td>
<td>It finds the domains who have conducted rollover.</td>
<td>the output of <code>rollover_candidate.py</code></td>
<td>-</td>
<td></td>
</tr>
<tr>
<td><code>rollover-stat.py</code></td>
<td>It finds domains that actually conduct rollover (e.g., except SMTP servers that never have valid TLSA records).</td>
<td>the output of <code>rollover.py</code></td>
<td>-</td>
<td></td>
</tr>
<tr>
<td><code>rollover-timeline-le.py</code></td>
<td>It evaluates rollover behaviors of SMTP servers that use certificates issued by Let&rsquo;s Encrypt.</td>
<td>the output of <code>rollover.py</code>, <code>le_stat_spark.py</code>, <code>rollover-stat.py</code></td>
<td><code>rollover_timeline_le.tar.gz</code></td>
<td></td>
</tr>
<tr>
<td><code>rollover-case.py</code></td>
<td>It evaluates rollover behaviors according to managing categories.</td>
<td>the output of <code>rollover.py</code>, <code>rollover_case_target.py</code>, <code>rollover-stat.py</code></td>
<td>-</td>
<td></td>
</tr>
<tr>
<td><code>gen-init-seed.py</code></td>
<td>It generates a set of SMTP servers that newly published TLSA records during our measurement period.</td>
<td>the output of <code>check_incorrect_reason.py</code>, <code>antago_syix.py</code>, <code>all-mx-exclude-nl.tar.gz</code></td>
<td>-</td>
<td></td>
</tr>
<tr>
<td><code>init-deploy-stat.py</code></td>
<td>It calculates stats of the initial DANE deployment.</td>
<td>the output of <code>check_incorrect_reason.py</code>, <code>init_deploy.py</code></td>
<td>-</td>
<td></td>
</tr>
</tbody>
</table>

<style>
table, th, td {
  text-align: center;
}
</style>

<h1 id="3-running-our-measurement-codes-to-get-your-own-raw-datasets-tlsa-records-and-their-certificates">3. Running our measurement codes to get your own raw datasets (TLSA records and their certificates)</h1>
<p>This section introduces our source codes that we used to collect our datasets. We used these source codes to collect TLSA records and their certificates chains every hour from July 13, 2019 to February 12, 2021. We refer to these measurements as the Hourly dataset (Section 4 in the paper).</p>
<p>What about Daily dataset? Because the Daily dataset that contains every domain names under top level domains was collected using zone files that are given under agreement with registries, we cannot make them just publicly available. Instead, we can provide intermediary data extracted from the Daily dataset which is needed to run our scripts. If you need the intermediary data, please email us for data access.</p>
<p>The source codes and how to use them are the same as the artifacts of the USENIX Security'20 paper. You can refer to <a href="https://dane-study.github.io/security2020/server-side/#section3">Server-side Artifacts / Section 3</a>.</p>
<!--
## Scanning source codes





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Filename</th>
<th>Descrption</th>
<th>Download</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>tlsa-scan.go</code></td>
<td>It fetches TLSA records from a list of domains</td>
<td><a href="/codes/tlsa-scan.go">link</a><sup>1</sup></td>
</tr>
<tr>
<td><code>starttls-scan.go</code></td>
<td>It collects certificates via STARTTLS</td>
<td><a href="/codes/starttls-scan.go">link</a></td>
</tr>
</tbody>
</table>

<style>
table, th, td {
  text-align: center;
}
</style>

<sup>1</sup> This requires the following third-party libraries: [Unbound](https://nlnetlabs.nl/projects/unbound/about/),  [Unbound Golang Wrapper](https://github.com/miekg/unbound), and [ldns](https://nlnetlabs.nl/projects/ldns/about/).


## How to scan TLSA records and their certificates

### 1. Scan TLSA records

The script `tlsa-scan.go` will read a list of domains in the `mx-with-tlsa` file and collect their TLSA records. The output has the following format.






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>TLSA-base-domain</th>
<th>Vantage point</th>
<th>DNSSEC validity <sup>1</sup></th>
<th>TLSA records<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td>_25._tcp.mail.ietf.org.</td>
<td>Virginia</td>
<td>Secure</td>
<td>AACBoAABAAIAB&hellip;</td>
</tr>
<tr>
<td>_25._tcp.mail.tutanota.de.</td>
<td>Virginia</td>
<td>Secure</td>
<td>AACBoAABAAIA&hellip;</td>
</tr>
<tr>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>

<style>
table, th, td {
  text-align: center;
}
</style>


<sup>1</sup> The result of DNSSEC validation: `Secure` indicates that a domain can be validated. `Insecure` indicates that a domain cannot be validated because it does not have a DS record. `Bogus` indicates that a domain cannot be validated because it has invalid DNSSEC records such as expired RRSIGs.

<sup>2</sup> TLSA records: wire formatted TLSA records (base64 encoded)

### 2. Scan STARTTLS certificates
The script `starttls-scan.go` will read a list of domains in the `mx-with-tlsa` file and collect certificates presented via STARTTLS. The output has the following format.






<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Domains</th>
<th>Port</th>
<th>Vantage point</th>
<th>Status<sup>1</sup></th>
<th># of presented certificates<sup>2</sup></th>
<th>Certificates<sup>3</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td>mail.ietf.org</td>
<td>25</td>
<td>Virginia</td>
<td>Success</td>
<td>4</td>
<td>LS0RUaAB&hellip;, WjGdVBWYi&hellip;, 0s3FTFRuZ1&hellip;, eFKdDRBO&hellip;</td>
</tr>
<tr>
<td>mail.tutanota.de.</td>
<td>25</td>
<td>Virginia</td>
<td>Success</td>
<td>4</td>
<td>LSSf7JanC&hellip;, ODlF4NEF&hellip;, SA3S29K&hellip;, Z1RstKS&hellip;</td>
</tr>
<tr>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>

<style>
table, th, td {
  text-align: center;
}
</style>


<sup>1</sup> Whether a certificate has been successfully fetched or not.

<sup>2</sup> The number of certificates presented via STARTTLS.  

<sup>3</sup> A list of base64 encoded (PEM format) certificates (each certificate is comma seperated).
-->

    </main>
    <footer class="container-fluid page-footer" style="display: flex; align-items: center">
</footer>

    
<script type="text/javascript">
var sc_project=11603023; 
var sc_invisible=1; 
var sc_security="a906280a"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11603023/0/a906280a/1/" alt="Web
Analytics"></a></div></noscript>


    
    <script src="js/jquery.1.12.4.min.js"></script>
    
    <script src="js/bootstrap.min.js"></script>
    <script src="js/script.js"></script>
  </body>
</html>
